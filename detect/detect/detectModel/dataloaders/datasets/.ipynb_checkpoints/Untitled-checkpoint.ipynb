{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize a tensor image with mean and standard deviation.\n",
    "    Args:\n",
    "        mean (tuple): means for each channel.\n",
    "        std (tuple): standard deviations for each channel.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=(0., 0., 0.), std=(1., 1., 1.)):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "        img = np.array(img).astype(np.float32)\n",
    "        mask = np.array(mask).astype(np.float32)\n",
    "        img /= 255.0\n",
    "        img -= self.mean\n",
    "        img /= self.std\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "        img = np.array(img).astype(np.float32).transpose((2, 0, 1))\n",
    "        mask = np.array(mask).astype(np.float32)\n",
    "\n",
    "        img = torch.from_numpy(img).float()\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "        if random.random() < 0.5:\n",
    "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "\n",
    "\n",
    "class RandomRotate(object):\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "        rotate_degree = random.uniform(-1*self.degree, self.degree)\n",
    "        img = img.rotate(rotate_degree, Image.BILINEAR)\n",
    "        mask = mask.rotate(rotate_degree, Image.NEAREST)\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "\n",
    "\n",
    "class RandomGaussianBlur(object):\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "        if random.random() < 0.5:\n",
    "            img = img.filter(ImageFilter.GaussianBlur(\n",
    "                radius=random.random()))\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "\n",
    "\n",
    "class RandomScaleCrop(object):\n",
    "    def __init__(self, base_size, crop_size, fill=0):\n",
    "        self.base_size = base_size\n",
    "        self.crop_size = crop_size\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "        # random scale (short edge)\n",
    "        short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))\n",
    "        w, h = img.size\n",
    "        if h > w:\n",
    "            ow = short_size\n",
    "            oh = int(1.0 * h * ow / w)\n",
    "        else:\n",
    "            oh = short_size\n",
    "            ow = int(1.0 * w * oh / h)\n",
    "        img = img.resize((ow, oh), Image.BILINEAR)\n",
    "        mask = mask.resize((ow, oh), Image.NEAREST)\n",
    "        # pad crop\n",
    "        if short_size < self.crop_size:\n",
    "            padh = self.crop_size - oh if oh < self.crop_size else 0\n",
    "            padw = self.crop_size - ow if ow < self.crop_size else 0\n",
    "            img = ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)\n",
    "            mask = ImageOps.expand(mask, border=(0, 0, padw, padh), fill=self.fill)\n",
    "        # random crop crop_size\n",
    "        w, h = img.size\n",
    "        x1 = random.randint(0, w - self.crop_size)\n",
    "        y1 = random.randint(0, h - self.crop_size)\n",
    "        img = img.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n",
    "        mask = mask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "\n",
    "\n",
    "class FixScaleCrop(object):\n",
    "    def __init__(self, crop_size):\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "        w, h = img.size\n",
    "        if w > h:\n",
    "            oh = self.crop_size\n",
    "            ow = int(1.0 * w * oh / h)\n",
    "        else:\n",
    "            ow = self.crop_size\n",
    "            oh = int(1.0 * h * ow / w)\n",
    "        img = img.resize((ow, oh), Image.BILINEAR)\n",
    "        mask = mask.resize((ow, oh), Image.NEAREST)\n",
    "        # center crop\n",
    "        w, h = img.size\n",
    "        x1 = int(round((w - self.crop_size) / 2.))\n",
    "        y1 = int(round((h - self.crop_size) / 2.))\n",
    "        img = img.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n",
    "        mask = mask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "\n",
    "class FixedResize(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = (size, size)  # size: (h, w)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "\n",
    "        assert img.size == mask.size\n",
    "\n",
    "        img = img.resize(self.size, Image.BILINEAR)\n",
    "        mask = mask.resize(self.size, Image.NEAREST)\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def decode_seg_map_sequence(label_masks, dataset='pascal'):\n",
    "    rgb_masks = []\n",
    "    for label_mask in label_masks:\n",
    "        rgb_mask = decode_segmap(label_mask, dataset)\n",
    "        rgb_masks.append(rgb_mask)\n",
    "    rgb_masks = torch.from_numpy(np.array(rgb_masks).transpose([0, 3, 1, 2]))\n",
    "    return rgb_masks\n",
    "\n",
    "\n",
    "def decode_segmap(label_mask, dataset, plot=False):\n",
    "    \"\"\"Decode segmentation class labels into a color image\n",
    "    Args:\n",
    "        label_mask (np.ndarray): an (M,N) array of integer values denoting\n",
    "          the class label at each spatial location.\n",
    "        plot (bool, optional): whether to show the resulting color image\n",
    "          in a figure.\n",
    "    Returns:\n",
    "        (np.ndarray, optional): the resulting decoded color image.\n",
    "    \"\"\"\n",
    "    if dataset == 'pascal' or dataset == 'coco':\n",
    "        n_classes = 21\n",
    "        label_colours = get_pascal_labels()\n",
    "    elif dataset == 'cityscapes':\n",
    "        n_classes = 19\n",
    "        label_colours = get_cityscapes_labels()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    r = label_mask.copy()\n",
    "    g = label_mask.copy()\n",
    "    b = label_mask.copy()\n",
    "    for ll in range(0, n_classes):\n",
    "        r[label_mask == ll] = label_colours[ll, 0]\n",
    "        g[label_mask == ll] = label_colours[ll, 1]\n",
    "        b[label_mask == ll] = label_colours[ll, 2]\n",
    "    rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\n",
    "    rgb[:, :, 0] = r / 255.0\n",
    "    rgb[:, :, 1] = g / 255.0\n",
    "    rgb[:, :, 2] = b / 255.0\n",
    "    if plot:\n",
    "        plt.imshow(rgb)\n",
    "        plt.show()\n",
    "    else:\n",
    "        return rgb\n",
    "\n",
    "\n",
    "def encode_segmap(mask):\n",
    "    \"\"\"Encode segmentation label images as pascal classes\n",
    "    Args:\n",
    "        mask (np.ndarray): raw segmentation label image of dimension\n",
    "          (M, N, 3), in which the Pascal classes are encoded as colours.\n",
    "    Returns:\n",
    "        (np.ndarray): class map with dimensions (M,N), where the value at\n",
    "        a given location is the integer denoting the class index.\n",
    "    \"\"\"\n",
    "    mask = mask.astype(int)\n",
    "    label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int16)\n",
    "    for ii, label in enumerate(get_pascal_labels()):\n",
    "        label_mask[np.where(np.all(mask == label, axis=-1))[:2]] = ii\n",
    "    label_mask = label_mask.astype(int)\n",
    "    return label_mask\n",
    "\n",
    "\n",
    "def get_cityscapes_labels():\n",
    "    return np.array([\n",
    "        [128, 64, 128],\n",
    "        [244, 35, 232],\n",
    "        [70, 70, 70],\n",
    "        [102, 102, 156],\n",
    "        [190, 153, 153],\n",
    "        [153, 153, 153],\n",
    "        [250, 170, 30],\n",
    "        [220, 220, 0],\n",
    "        [107, 142, 35],\n",
    "        [152, 251, 152],\n",
    "        [0, 130, 180],\n",
    "        [220, 20, 60],\n",
    "        [255, 0, 0],\n",
    "        [0, 0, 142],\n",
    "        [0, 0, 70],\n",
    "        [0, 60, 100],\n",
    "        [0, 80, 100],\n",
    "        [0, 0, 230],\n",
    "        [119, 11, 32]])\n",
    "\n",
    "\n",
    "def get_pascal_labels():\n",
    "    \"\"\"Load the mapping that associates pascal classes with label colors\n",
    "    Returns:\n",
    "        np.ndarray with dimensions (21, 3)\n",
    "    \"\"\"\n",
    "    return np.asarray([[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n",
    "                       [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n",
    "                       [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n",
    "                       [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n",
    "                       [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
    "                       [0, 64, 128]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-f87655df-753d-4995-baf4-c259df46fe13.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import trange\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "class COCOSegmentation(Dataset):\n",
    "#     NUM_CLASSES = 21\n",
    "#     CAT_LIST = [0, 5, 2, 16, 9, 44, 6, 3, 17, 62, 21, 67, 18, 19, 4,\n",
    "#         1, 64, 20, 63, 7, 72]\n",
    "\n",
    "    NUM_CLASSES = 32\n",
    "    CAT_LIST = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,\n",
    "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32]\n",
    "\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 base_dir='/data/data/P1-sum-pro/',\n",
    "                 split='train',\n",
    "                 year='2017'):\n",
    "        super().__init__()\n",
    "#         ann_file = os.path.join(base_dir, 'annotations/instances_{}{}.json'.format(split, year))\n",
    "#         ids_file = os.path.join(base_dir, 'annotations/{}_ids_{}.pth'.format(split, year))\n",
    "#         self.img_dir = os.path.join(base_dir, 'images/{}{}'.format(split, year))\n",
    "        ann_file = os.path.join(base_dir, 'annotations/{}.json'.format(split))\n",
    "        ids_file = os.path.join(base_dir, 'annotations/{}_ids.pth'.format(split))\n",
    "        self.img_dir = os.path.join(base_dir, 'images/{}'.format(split))\n",
    "        self.split = split\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.coco_mask = mask\n",
    "        if os.path.exists(ids_file):\n",
    "            self.ids = torch.load(ids_file)\n",
    "        else:\n",
    "            ids = list(self.coco.imgs.keys())\n",
    "            self.ids = self._preprocess(ids, ids_file)\n",
    "        self.args = args\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        _img, _target = self._make_img_gt_point_pair(index)\n",
    "        sample = {'image': _img, 'label': _target}\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            return self.transform_tr(sample)\n",
    "        elif self.split == 'val':\n",
    "            return self.transform_val(sample)\n",
    "\n",
    "    def _make_img_gt_point_pair(self, index):\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        img_metadata = coco.loadImgs(img_id)[0]\n",
    "        path = img_metadata['file_name']\n",
    "        _img = Image.open(os.path.join(self.img_dir, path)).convert('RGB')\n",
    "        cocotarget = coco.loadAnns(coco.getAnnIds(imgIds=img_id))\n",
    "        _target = Image.fromarray(self._gen_seg_mask(\n",
    "            cocotarget, img_metadata['height'], img_metadata['width']))\n",
    "\n",
    "        return _img, _target\n",
    "\n",
    "    def _preprocess(self, ids, ids_file):\n",
    "        print(\"Preprocessing mask, this will take a while. \" + \\\n",
    "              \"But don't worry, it only run once for each split.\")\n",
    "        tbar = trange(len(ids))\n",
    "        new_ids = []\n",
    "        for i in tbar:\n",
    "            img_id = ids[i]\n",
    "            cocotarget = self.coco.loadAnns(self.coco.getAnnIds(imgIds=img_id))\n",
    "            img_metadata = self.coco.loadImgs(img_id)[0]\n",
    "            mask = self._gen_seg_mask(cocotarget, img_metadata['height'],\n",
    "                                      img_metadata['width'])\n",
    "            # more than 1k pixels\n",
    "            if (mask > 0).sum() > 1000:\n",
    "                new_ids.append(img_id)\n",
    "            tbar.set_description('Doing: {}/{}, got {} qualified images'. \\\n",
    "                                 format(i, len(ids), len(new_ids)))\n",
    "        print('Found number of qualified images: ', len(new_ids))\n",
    "        torch.save(new_ids, ids_file)\n",
    "        return new_ids\n",
    "\n",
    "    def _gen_seg_mask(self, target, h, w):\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        coco_mask = self.coco_mask\n",
    "        for instance in target:\n",
    "            rle = coco_mask.frPyObjects(instance['segmentation'], h, w)\n",
    "            m = coco_mask.decode(rle)\n",
    "            cat = instance['category_id']\n",
    "            if cat in self.CAT_LIST:\n",
    "                c = self.CAT_LIST.index(cat)\n",
    "            else:\n",
    "                continue\n",
    "            if len(m.shape) < 3:\n",
    "                mask[:, :] += (mask == 0) * (m * c)\n",
    "            else:\n",
    "                mask[:, :] += (mask == 0) * (((np.sum(m, axis=2)) > 0) * c).astype(np.uint8)\n",
    "        return mask\n",
    "\n",
    "    def transform_tr(self, sample):\n",
    "        composed_transforms = transforms.Compose([\n",
    "            RandomHorizontalFlip(),\n",
    "            RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size),\n",
    "            RandomGaussianBlur(),\n",
    "            Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensor()])\n",
    "\n",
    "        return composed_transforms(sample)\n",
    "\n",
    "    def transform_val(self, sample):\n",
    "\n",
    "        composed_transforms = transforms.Compose([\n",
    "            tr.FixScaleCrop(crop_size=self.args.crop_size),\n",
    "            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            tr.ToTensor()])\n",
    "\n",
    "        return composed_transforms(sample)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torchvision import transforms\n",
    "    import matplotlib.pyplot as plt\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    args = parser.parse_args()\n",
    "    args.base_size = 513\n",
    "    args.crop_size = 513\n",
    "\n",
    "    coco_val = COCOSegmentation(args, split='val', year='2017')\n",
    "\n",
    "    dataloader = DataLoader(coco_val, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "    for ii, sample in enumerate(dataloader):\n",
    "        for jj in range(sample[\"image\"].size()[0]):\n",
    "            img = sample['image'].numpy()\n",
    "#             gt = sample['label'].numpy()\n",
    "#             tmp = np.array(gt[jj]).astype(np.uint8)\n",
    "#             segmap = decode_segmap(tmp, dataset='coco')\n",
    "            img_tmp = np.transpose(img[jj], axes=[1, 2, 0])\n",
    "            img_tmp *= (0.229, 0.224, 0.225)\n",
    "            img_tmp += (0.485, 0.456, 0.406)\n",
    "            img_tmp *= 255.0\n",
    "            img_tmp = img_tmp.astype(np.uint8)\n",
    "            plt.figure()\n",
    "            plt.title('display')\n",
    "            plt.subplot(211)\n",
    "            plt.imshow(img_tmp)\n",
    "            plt.subplot(212)\n",
    "            plt.imshow(segmap)\n",
    "\n",
    "        if ii == 1:\n",
    "            break\n",
    "\n",
    "    plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
